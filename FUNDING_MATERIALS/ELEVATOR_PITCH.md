# PROACTIVE AI Constitution Toolkit

**Pitches written so anyone can understand.**

---

## One sentence (anyone)

PROACTIVE stops AI from saying things are "done" or "certain" when they aren't—so people don't act on false confidence.

## Three sentences (anyone)

When an AI says something is finished or true and it isn't, people get hurt—whether the AI meant to mislead or not. PROACTIVE is a set of six rules the AI can't skip: every claim gets checked before it reaches you. We tested it on 200 tricky questions; safe behavior tripled and the AI said "I don't know" 14× more often.

## Thirty seconds (anyone)

People rely on AI for real decisions. When the AI sounds sure but is wrong—"I finished that," "This code is safe," "That file exists"—the harm is real. Today we treat that as a "quality" problem. We think it's a safety problem: confident and wrong is as bad as lying.

PROACTIVE is a layer that checks every AI response against six simple rules (e.g. don't claim work is done without proof; say "I don't know" when you don't know). We plug into tools teams already use. We ran 200 questions; with PROACTIVE, safe behavior tripled. We're building this so AI can't confidently say things it can't back up.

---

## One-Sentence Pitch (technical)

PROACTIVE is a constitutional safety framework that makes AI reliability failures detectable and traceable for safety researchers, ML engineers, and organizations deploying AI systems where errors cause real harm.

## Three-Sentence Pitch (technical)

When AI systems make confident false claims that users act upon, the resulting harm is operationally indistinguishable from malice—yet current tooling treats these as mere quality issues. PROACTIVE provides nine enforceable behavioral constraints and a trace chain that links every AI decision to verifiable evidence. The toolkit includes working adapters for W&B, GitHub Actions, and HELM that integrate PROACTIVE into existing ML workflows.

## Thirty-Second Pitch (technical)

AI systems increasingly make claims that users rely on for consequential decisions. When those claims are confident, false, and actionable, the harm is real regardless of intent. PROACTIVE enforces epistemic reliability through gates that cannot be bypassed, with every claim traced to evidence. The toolkit includes adapters that plug into W&B, GitHub Actions, and HELM. We ran validation on 200 questions (TruthfulQA); safe behavior tripled, uncertainty admission 14×. We're seeking funding to complete validation and publish.

## Keywords

- Epistemic reliability
- AI safety
- Constitutional AI
- Toolchain integration
- Failure mode taxonomy (F1-F5)
- Model-Based Systems Engineering (MBSE)
- Trace chain verification
- Safety case methodology

---

## V&T

- Created: 2026-01-19T05:41:00Z
- Status: COMPLETE
- Blocked by: nothing
