# PROACTIVE AI Constitution Toolkit

## One-Sentence Pitch

PROACTIVE is a constitutional safety framework that makes AI reliability failures detectable and traceable for safety researchers, ML engineers, and organizations deploying AI systems where errors cause real harm.

## Three-Sentence Pitch

When AI systems make confident false claims that users act upon, the resulting harm is operationally indistinguishable from malice—yet current tooling treats these as mere quality issues. PROACTIVE provides a constitutional framework with nine enforceable behavioral constraints (the PROACTIVE mnemonic) and a trace chain that links every AI decision to verifiable evidence. The toolkit includes working adapters for W&B, GitHub Actions, and HELM that integrate PROACTIVE principles into existing ML workflows without requiring infrastructure overhaul.

## Thirty-Second Pitch

AI systems increasingly make claims that users rely on for consequential decisions. When those claims are confident, false, and actionable, the harm is real regardless of intent—a doctor misled by a confident hallucination, a developer shipping vulnerable code because the AI said it was safe. Current approaches treat reliability as a quality metric, not a safety requirement.

PROACTIVE changes this. It's a constitutional framework where epistemic reliability is enforced through gates that cannot be bypassed, with every claim traced to evidence. The toolkit includes adapters that plug into W&B, GitHub Actions, and HELM—tools teams already use. We're seeking funding to complete validation studies and publish findings that could shift how the field thinks about AI reliability failures.

## Keywords

- Epistemic reliability
- AI safety
- Constitutional AI
- Toolchain integration
- Failure mode taxonomy (F1-F5)
- Model-Based Systems Engineering (MBSE)
- Trace chain verification
- Safety case methodology

---

## V&T

- Created: 2026-01-19T05:41:00Z
- Status: COMPLETE
- Blocked by: nothing
